{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as l\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import tensorflow.keras.optimizers as o\n",
    "import tensorflow.keras.models as m\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data sets\n",
    "- MNIST\n",
    "- eMNIST\n",
    "- my custom one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mnist\n",
    "mnist = keras.datasets.mnist \n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "mnist_x_train = np.asarray(mnist_x_train)\n",
    "mnist_x_test = np.asarray(mnist_x_test)\n",
    "\n",
    "mnist_y_train = mnist_y_train.reshape((mnist_y_train.shape[0], 1))\n",
    "mnist_y_test = mnist_y_test.reshape((mnist_y_test.shape[0], 1))\n",
    "\n",
    "\n",
    "plt.imshow(mnist_x_train[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emnist\n",
    "from scipy.io import loadmat\n",
    "\n",
    "EMNIST_digits_file = os.path.join(\"..\", \"number_dataset\",\"emnist-digits.mat\")\n",
    "mat = loadmat(EMNIST_digits_file)\n",
    "data = mat['dataset']\n",
    "\n",
    "emnist_x_train = data['train'][0,0]['images'][0,0]\n",
    "emnist_x_test = data['test'][0,0]['images'][0,0]\n",
    "emnist_x_train = emnist_x_train.reshape( (emnist_x_train.shape[0], 28, 28), order='F')\n",
    "emnist_x_test = emnist_x_test.reshape( (emnist_x_test.shape[0], 28, 28), order='F')\n",
    "\n",
    "emnist_y_train = data['train'][0,0]['labels'][0,0]\n",
    "emnist_y_test = data['test'][0,0]['labels'][0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom one\n",
    "custom_dataset_path = os.path.join(\"..\", \"number_dataset\", \"COMPLETE_DATASET_clean\")\n",
    "custom_dataset_names = os.listdir(custom_dataset_path)\n",
    "\n",
    "IMG_SIZE = 28\n",
    "custom_dataset = []\n",
    "custom_labels = []\n",
    "\n",
    "for i,img_name in enumerate(custom_dataset_names):\n",
    "    # load image gray scale\n",
    "    img = cv2.imread(os.path.join(custom_dataset_path,img_name), cv2.IMREAD_GRAYSCALE)\n",
    "    # resize to 28x28\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE), 1)\n",
    "    #thress hold it \n",
    "    ret, t_img = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY_INV)# THRESH_TOZERO_INV or THRESH_BINARY_INV\n",
    "    ret, img = cv2.threshold(img, 200, 255, cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "    # append it\n",
    "    custom_dataset.append(img)\n",
    "\n",
    "    # label\n",
    "    current_label = np.zeros(10)\n",
    "    current_label[int(img_name[0])]=1\n",
    "    custom_labels.append(current_label)\n",
    "    \n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "my_seed = int(10*random.random())\n",
    "random.Random(my_seed).shuffle(custom_dataset)\n",
    "random.Random(my_seed).shuffle(custom_labels)\n",
    "\n",
    "custom_x_train = np.asarray(custom_dataset[:int(0.75*len(custom_dataset))])\n",
    "custom_y_train = np.asarray(custom_labels[:int(0.75*len(custom_labels))])\n",
    "\n",
    "custom_x_test = np.asarray(custom_dataset[int(0.75*len(custom_dataset)):])\n",
    "custom_y_test = np.asarray(custom_labels[int(0.75*len(custom_labels)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240000, 1)\n",
      "(60000, 1)\n",
      "(1266, 10)\n"
     ]
    }
   ],
   "source": [
    "print(emnist_y_train.shape)\n",
    "print(mnist_y_train.shape)\n",
    "print(custom_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat them all\n",
    "\n",
    "x_train, x_test = np.concatenate((emnist_x_train, mnist_x_train)), np.concatenate((emnist_x_test, mnist_x_test))\n",
    "x_train, x_test = np.concatenate((x_train, custom_x_train)), np.concatenate((x_test, custom_x_test))\n",
    "\n",
    "# after augmentation\n",
    "# x_train, x_test = ((1.0/255.0)*x_train).reshape([-1, 28, 28, 1]), ((1.0/255.0)*x_test).reshape([-1, 28, 28, 1]) \n",
    "\n",
    "y_train, y_test = np.concatenate((emnist_y_train, mnist_y_train)), np.concatenate((emnist_y_test, mnist_y_test))\n",
    "y_train, y_test = keras.utils.to_categorical(y_train, 10), keras.utils.to_categorical(y_test, 10)\n",
    "y_train, y_test = np.concatenate((y_train, custom_y_train)), np.concatenate((y_test, custom_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301266, 28, 28)\n",
      "(301266, 10)\n",
      "(50422, 28, 28)\n",
      "(50422, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some augmentated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirro_rotate(dataset, dataset_labels, ROT_DEGGREES=20):\n",
    "    augmented_image = []\n",
    "    augmented_image_labels = []\n",
    "    \n",
    "    for num in range (0, dataset.shape[0]):\n",
    "        # original image:\n",
    "        augmented_image.append(dataset[num])\n",
    "        augmented_image_labels.append(dataset_labels[num])\n",
    "        #rotate \n",
    "        height, width = dataset[num].shape\n",
    "        \n",
    "        rotation_matrix = cv2.getRotationMatrix2D((width/2,height/2), ROT_DEGGREES ,1)\n",
    "        rotated_image = cv2.warpAffine(dataset[num], rotation_matrix, (width, height))\n",
    "        augmented_image.append(rotated_image)\n",
    "        augmented_image_labels.append(dataset_labels[num])\n",
    "        \n",
    "        rotation_matrix = cv2.getRotationMatrix2D((width/2,height/2), (-1)*ROT_DEGGREES ,1)\n",
    "        rotated_image = cv2.warpAffine(dataset[num], rotation_matrix, (width, height))\n",
    "        augmented_image.append(rotated_image)\n",
    "        augmented_image_labels.append(dataset_labels[num])\n",
    "    \n",
    "    return np.array(augmented_image), np.array(augmented_image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903798, 28, 28)\n",
      "(903798, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = mirro_rotate(x_train, y_train, 10)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = ((1.0/255.0)*x_train).reshape([-1, 28, 28, 1]), ((1.0/255.0)*x_test).reshape([-1, 28, 28, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 49)        490       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 9, 9, 49)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 36)          15912     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 3, 36)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 324)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                10400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 29,564\n",
      "Trainable params: 29,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = m.Sequential()\n",
    "\n",
    "model.add(l.Conv2D(49, (3,3), padding='same', activation='relu', input_shape = (28,28,1)))\n",
    "model.add(l.MaxPooling2D((3,3)))\n",
    "model.add(l.Conv2D(36, (3,3), padding='same', activation='relu'))\n",
    "model.add(l.MaxPooling2D((3,3)))\n",
    "\n",
    "model.add(l.Flatten())\n",
    "model.add(l.Dense(32, activation=\"relu\"))\n",
    "model.add(l.Dropout(rate=0.5))\n",
    "model.add(l.Dense(64, activation=\"relu\"))\n",
    "\n",
    "model.add(l.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CallBacks: TensorBoard + EarlyStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lon_name:  tf_1579361220.log\n"
     ]
    }
   ],
   "source": [
    "name = \"tf_{}.log\".format(int(time.time()))\n",
    "print(\"lon_name: \",name)\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(name))\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=1, patience=0,\n",
    "    verbose=0, mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 903798 samples, validate on 50422 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26656/903798 [..............................] - ETA: 8:42:49 - loss: 2.3105 - accuracy: 0.093 - ETA: 3:03:31 - loss: 2.2962 - accuracy: 0.093 - ETA: 1:22:04 - loss: 2.2863 - accuracy: 0.107 - ETA: 50:07 - loss: 2.2903 - accuracy: 0.1042  - ETA: 36:58 - loss: 2.2928 - accuracy: 0.110 - ETA: 29:46 - loss: 2.2918 - accuracy: 0.099 - ETA: 25:12 - loss: 2.2884 - accuracy: 0.105 - ETA: 22:05 - loss: 2.2848 - accuracy: 0.114 - ETA: 19:45 - loss: 2.2771 - accuracy: 0.123 - ETA: 18:01 - loss: 2.2693 - accuracy: 0.130 - ETA: 16:41 - loss: 2.2600 - accuracy: 0.139 - ETA: 15:35 - loss: 2.2449 - accuracy: 0.149 - ETA: 14:42 - loss: 2.2300 - accuracy: 0.160 - ETA: 13:57 - loss: 2.2137 - accuracy: 0.167 - ETA: 13:19 - loss: 2.1932 - accuracy: 0.177 - ETA: 12:45 - loss: 2.1682 - accuracy: 0.187 - ETA: 12:15 - loss: 2.1480 - accuracy: 0.198 - ETA: 11:49 - loss: 2.1259 - accuracy: 0.205 - ETA: 11:26 - loss: 2.1028 - accuracy: 0.215 - ETA: 11:12 - loss: 2.0841 - accuracy: 0.224 - ETA: 10:52 - loss: 2.0534 - accuracy: 0.236 - ETA: 10:36 - loss: 2.0297 - accuracy: 0.246 - ETA: 10:21 - loss: 2.0067 - accuracy: 0.257 - ETA: 10:08 - loss: 1.9801 - accuracy: 0.266 - ETA: 9:59 - loss: 1.9574 - accuracy: 0.276 - ETA: 9:52 - loss: 1.9347 - accuracy: 0.28 - ETA: 9:41 - loss: 1.9106 - accuracy: 0.29 - ETA: 9:34 - loss: 1.8915 - accuracy: 0.30 - ETA: 9:25 - loss: 1.8652 - accuracy: 0.31 - ETA: 9:19 - loss: 1.8562 - accuracy: 0.31 - ETA: 9:15 - loss: 1.8384 - accuracy: 0.32 - ETA: 9:06 - loss: 1.8182 - accuracy: 0.33 - ETA: 8:58 - loss: 1.7969 - accuracy: 0.34 - ETA: 8:51 - loss: 1.7783 - accuracy: 0.34 - ETA: 8:45 - loss: 1.7582 - accuracy: 0.35 - ETA: 8:40 - loss: 1.7405 - accuracy: 0.36 - ETA: 8:35 - loss: 1.7233 - accuracy: 0.36 - ETA: 8:29 - loss: 1.7022 - accuracy: 0.37 - ETA: 8:24 - loss: 1.6871 - accuracy: 0.38 - ETA: 8:19 - loss: 1.6702 - accuracy: 0.38 - ETA: 8:15 - loss: 1.6564 - accuracy: 0.39 - ETA: 8:11 - loss: 1.6399 - accuracy: 0.40 - ETA: 8:07 - loss: 1.6253 - accuracy: 0.40 - ETA: 8:03 - loss: 1.6094 - accuracy: 0.41 - ETA: 7:59 - loss: 1.5903 - accuracy: 0.41 - ETA: 7:57 - loss: 1.5816 - accuracy: 0.42 - ETA: 7:53 - loss: 1.5686 - accuracy: 0.42 - ETA: 7:51 - loss: 1.5600 - accuracy: 0.43 - ETA: 7:49 - loss: 1.5519 - accuracy: 0.43 - ETA: 7:45 - loss: 1.5407 - accuracy: 0.43 - ETA: 7:42 - loss: 1.5284 - accuracy: 0.44 - ETA: 7:39 - loss: 1.5175 - accuracy: 0.44 - ETA: 7:37 - loss: 1.5025 - accuracy: 0.45 - ETA: 7:34 - loss: 1.4917 - accuracy: 0.45 - ETA: 7:31 - loss: 1.4823 - accuracy: 0.45 - ETA: 7:29 - loss: 1.4720 - accuracy: 0.46 - ETA: 7:26 - loss: 1.4624 - accuracy: 0.46 - ETA: 7:24 - loss: 1.4562 - accuracy: 0.47 - ETA: 7:22 - loss: 1.4462 - accuracy: 0.47 - ETA: 7:20 - loss: 1.4393 - accuracy: 0.47 - ETA: 7:19 - loss: 1.4313 - accuracy: 0.47 - ETA: 7:16 - loss: 1.4212 - accuracy: 0.48 - ETA: 7:15 - loss: 1.4149 - accuracy: 0.48 - ETA: 7:14 - loss: 1.4066 - accuracy: 0.48 - ETA: 7:12 - loss: 1.3994 - accuracy: 0.48 - ETA: 7:10 - loss: 1.3907 - accuracy: 0.49 - ETA: 7:09 - loss: 1.3823 - accuracy: 0.49 - ETA: 7:07 - loss: 1.3752 - accuracy: 0.49 - ETA: 7:05 - loss: 1.3662 - accuracy: 0.50 - ETA: 7:03 - loss: 1.3592 - accuracy: 0.50 - ETA: 7:02 - loss: 1.3523 - accuracy: 0.50 - ETA: 7:00 - loss: 1.3444 - accuracy: 0.51 - ETA: 6:59 - loss: 1.3366 - accuracy: 0.51 - ETA: 6:57 - loss: 1.3296 - accuracy: 0.51 - ETA: 6:57 - loss: 1.3234 - accuracy: 0.51 - ETA: 6:56 - loss: 1.3166 - accuracy: 0.52 - ETA: 6:55 - loss: 1.3104 - accuracy: 0.52 - ETA: 6:53 - loss: 1.3045 - accuracy: 0.52 - ETA: 6:52 - loss: 1.2964 - accuracy: 0.52 - ETA: 6:50 - loss: 1.2876 - accuracy: 0.53 - ETA: 6:49 - loss: 1.2835 - accuracy: 0.53 - ETA: 6:48 - loss: 1.2763 - accuracy: 0.53 - ETA: 6:48 - loss: 1.2702 - accuracy: 0.53 - ETA: 6:46 - loss: 1.2627 - accuracy: 0.54 - ETA: 6:45 - loss: 1.2552 - accuracy: 0.54 - ETA: 6:44 - loss: 1.2493 - accuracy: 0.54 - ETA: 6:43 - loss: 1.2427 - accuracy: 0.54 - ETA: 6:42 - loss: 1.2346 - accuracy: 0.55 - ETA: 6:41 - loss: 1.2290 - accuracy: 0.55 - ETA: 6:40 - loss: 1.2236 - accuracy: 0.55 - ETA: 6:39 - loss: 1.2180 - accuracy: 0.55 - ETA: 6:39 - loss: 1.2136 - accuracy: 0.56 - ETA: 6:37 - loss: 1.2085 - accuracy: 0.56 - ETA: 6:37 - loss: 1.2045 - accuracy: 0.56 - ETA: 6:37 - loss: 1.2015 - accuracy: 0.56 - ETA: 6:37 - loss: 1.1984 - accuracy: 0.56 - ETA: 6:37 - loss: 1.1936 - accuracy: 0.56 - ETA: 6:36 - loss: 1.1895 - accuracy: 0.57 - ETA: 6:35 - loss: 1.1855 - accuracy: 0.57 - ETA: 6:35 - loss: 1.1831 - accuracy: 0.57 - ETA: 6:35 - loss: 1.1780 - accuracy: 0.57 - ETA: 6:35 - loss: 1.1748 - accuracy: 0.57 - ETA: 6:35 - loss: 1.1709 - accuracy: 0.57 - ETA: 6:34 - loss: 1.1664 - accuracy: 0.57 - ETA: 6:34 - loss: 1.1632 - accuracy: 0.58 - ETA: 6:33 - loss: 1.1593 - accuracy: 0.58 - ETA: 6:33 - loss: 1.1548 - accuracy: 0.58 - ETA: 6:32 - loss: 1.1508 - accuracy: 0.58 - ETA: 6:31 - loss: 1.1453 - accuracy: 0.58 - ETA: 6:30 - loss: 1.1409 - accuracy: 0.58 - ETA: 6:29 - loss: 1.1361 - accuracy: 0.59 - ETA: 6:30 - loss: 1.1339 - accuracy: 0.59 - ETA: 6:29 - loss: 1.1309 - accuracy: 0.59 - ETA: 6:29 - loss: 1.1278 - accuracy: 0.59 - ETA: 6:28 - loss: 1.1253 - accuracy: 0.59 - ETA: 6:28 - loss: 1.1219 - accuracy: 0.59 - ETA: 6:27 - loss: 1.1177 - accuracy: 0.59 - ETA: 6:27 - loss: 1.1134 - accuracy: 0.60 - ETA: 6:26 - loss: 1.1113 - accuracy: 0.60 - ETA: 6:26 - loss: 1.1076 - accuracy: 0.60 - ETA: 6:26 - loss: 1.1039 - accuracy: 0.60 - ETA: 6:26 - loss: 1.1024 - accuracy: 0.60 - ETA: 6:25 - loss: 1.0984 - accuracy: 0.60 - ETA: 6:25 - loss: 1.0960 - accuracy: 0.60 - ETA: 6:24 - loss: 1.0938 - accuracy: 0.60 - ETA: 6:24 - loss: 1.0907 - accuracy: 0.60 - ETA: 6:23 - loss: 1.0874 - accuracy: 0.61 - ETA: 6:23 - loss: 1.0844 - accuracy: 0.61 - ETA: 6:23 - loss: 1.0821 - accuracy: 0.61 - ETA: 6:23 - loss: 1.0793 - accuracy: 0.61 - ETA: 6:22 - loss: 1.0753 - accuracy: 0.61 - ETA: 6:22 - loss: 1.0728 - accuracy: 0.61 - ETA: 6:22 - loss: 1.0693 - accuracy: 0.61 - ETA: 6:22 - loss: 1.0669 - accuracy: 0.61 - ETA: 6:21 - loss: 1.0632 - accuracy: 0.61 - ETA: 6:21 - loss: 1.0585 - accuracy: 0.62 - ETA: 6:21 - loss: 1.0565 - accuracy: 0.62 - ETA: 6:21 - loss: 1.0539 - accuracy: 0.62 - ETA: 6:21 - loss: 1.0510 - accuracy: 0.62 - ETA: 6:20 - loss: 1.0482 - accuracy: 0.62 - ETA: 6:19 - loss: 1.0460 - accuracy: 0.62 - ETA: 6:20 - loss: 1.0432 - accuracy: 0.62 - ETA: 6:20 - loss: 1.0404 - accuracy: 0.62 - ETA: 6:20 - loss: 1.0382 - accuracy: 0.62 - ETA: 6:20 - loss: 1.0360 - accuracy: 0.62 - ETA: 6:20 - loss: 1.0334 - accuracy: 0.63 - ETA: 6:20 - loss: 1.0304 - accuracy: 0.63 - ETA: 6:19 - loss: 1.0263 - accuracy: 0.63 - ETA: 6:18 - loss: 1.0238 - accuracy: 0.63 - ETA: 6:18 - loss: 1.0211 - accuracy: 0.63 - ETA: 6:18 - loss: 1.0188 - accuracy: 0.63 - ETA: 6:18 - loss: 1.0157 - accuracy: 0.63 - ETA: 6:17 - loss: 1.0147 - accuracy: 0.63 - ETA: 6:17 - loss: 1.0121 - accuracy: 0.63 - ETA: 6:17 - loss: 1.0102 - accuracy: 0.63 - ETA: 6:17 - loss: 1.0080 - accuracy: 0.63 - ETA: 6:16 - loss: 1.0056 - accuracy: 0.64 - ETA: 6:16 - loss: 1.0029 - accuracy: 0.64 - ETA: 6:16 - loss: 0.9998 - accuracy: 0.64 - ETA: 6:15 - loss: 0.9982 - accuracy: 0.64 - ETA: 6:15 - loss: 0.9960 - accuracy: 0.64 - ETA: 6:14 - loss: 0.9932 - accuracy: 0.64 - ETA: 6:14 - loss: 0.9900 - accuracy: 0.64 - ETA: 6:14 - loss: 0.9871 - accuracy: 0.64 - ETA: 6:14 - loss: 0.9855 - accuracy: 0.64 - ETA: 6:13 - loss: 0.9830 - accuracy: 0.64 - ETA: 6:13 - loss: 0.9814 - accuracy: 0.64 - ETA: 6:13 - loss: 0.9792 - accuracy: 0.65 - ETA: 6:13 - loss: 0.9773 - accuracy: 0.65 - ETA: 6:13 - loss: 0.9749 - accuracy: 0.65 - ETA: 6:13 - loss: 0.9730 - accuracy: 0.65 - ETA: 6:12 - loss: 0.9696 - accuracy: 0.65 - ETA: 6:12 - loss: 0.9671 - accuracy: 0.65 - ETA: 6:12 - loss: 0.9647 - accuracy: 0.65 - ETA: 6:11 - loss: 0.9629 - accuracy: 0.65 - ETA: 6:11 - loss: 0.9611 - accuracy: 0.65 - ETA: 6:11 - loss: 0.9599 - accuracy: 0.65 - ETA: 6:11 - loss: 0.9571 - accuracy: 0.65 - ETA: 6:11 - loss: 0.9558 - accuracy: 0.65 - ETA: 6:11 - loss: 0.9541 - accuracy: 0.65 - ETA: 6:11 - loss: 0.9525 - accuracy: 0.66 - ETA: 6:11 - loss: 0.9510 - accuracy: 0.66 - ETA: 6:11 - loss: 0.9492 - accuracy: 0.66 54400/903798 [>.............................] - ETA: 6:10 - loss: 0.9474 - accuracy: 0.66 - ETA: 6:10 - loss: 0.9458 - accuracy: 0.66 - ETA: 6:10 - loss: 0.9441 - accuracy: 0.66 - ETA: 6:10 - loss: 0.9424 - accuracy: 0.66 - ETA: 6:10 - loss: 0.9404 - accuracy: 0.66 - ETA: 6:10 - loss: 0.9385 - accuracy: 0.66 - ETA: 6:10 - loss: 0.9372 - accuracy: 0.66 - ETA: 6:10 - loss: 0.9351 - accuracy: 0.66 - ETA: 6:10 - loss: 0.9328 - accuracy: 0.66 - ETA: 6:09 - loss: 0.9304 - accuracy: 0.66 - ETA: 6:09 - loss: 0.9287 - accuracy: 0.66 - ETA: 6:09 - loss: 0.9275 - accuracy: 0.66 - ETA: 6:09 - loss: 0.9247 - accuracy: 0.67 - ETA: 6:09 - loss: 0.9228 - accuracy: 0.67 - ETA: 6:08 - loss: 0.9202 - accuracy: 0.67 - ETA: 6:08 - loss: 0.9185 - accuracy: 0.67 - ETA: 6:08 - loss: 0.9167 - accuracy: 0.67 - ETA: 6:07 - loss: 0.9146 - accuracy: 0.67 - ETA: 6:07 - loss: 0.9126 - accuracy: 0.67 - ETA: 6:07 - loss: 0.9098 - accuracy: 0.67 - ETA: 6:06 - loss: 0.9079 - accuracy: 0.67 - ETA: 6:06 - loss: 0.9079 - accuracy: 0.67 - ETA: 6:06 - loss: 0.9062 - accuracy: 0.67 - ETA: 6:05 - loss: 0.9039 - accuracy: 0.67 - ETA: 6:05 - loss: 0.9022 - accuracy: 0.67 - ETA: 6:05 - loss: 0.9001 - accuracy: 0.67 - ETA: 6:04 - loss: 0.8983 - accuracy: 0.68 - ETA: 6:04 - loss: 0.8966 - accuracy: 0.68 - ETA: 6:04 - loss: 0.8948 - accuracy: 0.68 - ETA: 6:04 - loss: 0.8929 - accuracy: 0.68 - ETA: 6:04 - loss: 0.8916 - accuracy: 0.68 - ETA: 6:04 - loss: 0.8900 - accuracy: 0.68 - ETA: 6:04 - loss: 0.8877 - accuracy: 0.68 - ETA: 6:03 - loss: 0.8859 - accuracy: 0.68 - ETA: 6:03 - loss: 0.8845 - accuracy: 0.68 - ETA: 6:02 - loss: 0.8824 - accuracy: 0.68 - ETA: 6:02 - loss: 0.8810 - accuracy: 0.68 - ETA: 6:02 - loss: 0.8786 - accuracy: 0.68 - ETA: 6:02 - loss: 0.8767 - accuracy: 0.68 - ETA: 6:02 - loss: 0.8745 - accuracy: 0.68 - ETA: 6:01 - loss: 0.8728 - accuracy: 0.68 - ETA: 6:01 - loss: 0.8711 - accuracy: 0.68 - ETA: 6:01 - loss: 0.8688 - accuracy: 0.69 - ETA: 6:01 - loss: 0.8676 - accuracy: 0.69 - ETA: 6:00 - loss: 0.8659 - accuracy: 0.69 - ETA: 6:00 - loss: 0.8645 - accuracy: 0.69 - ETA: 6:00 - loss: 0.8633 - accuracy: 0.69 - ETA: 5:59 - loss: 0.8615 - accuracy: 0.69 - ETA: 5:59 - loss: 0.8601 - accuracy: 0.69 - ETA: 5:59 - loss: 0.8582 - accuracy: 0.69 - ETA: 5:58 - loss: 0.8569 - accuracy: 0.69 - ETA: 5:58 - loss: 0.8552 - accuracy: 0.69 - ETA: 5:58 - loss: 0.8537 - accuracy: 0.69 - ETA: 5:57 - loss: 0.8521 - accuracy: 0.69 - ETA: 5:57 - loss: 0.8509 - accuracy: 0.69 - ETA: 5:57 - loss: 0.8495 - accuracy: 0.69 - ETA: 5:57 - loss: 0.8481 - accuracy: 0.69 - ETA: 5:56 - loss: 0.8464 - accuracy: 0.69 - ETA: 5:56 - loss: 0.8453 - accuracy: 0.69 - ETA: 5:56 - loss: 0.8442 - accuracy: 0.69 - ETA: 5:56 - loss: 0.8426 - accuracy: 0.70 - ETA: 5:56 - loss: 0.8405 - accuracy: 0.70 - ETA: 5:56 - loss: 0.8396 - accuracy: 0.70 - ETA: 5:56 - loss: 0.8379 - accuracy: 0.70 - ETA: 5:55 - loss: 0.8368 - accuracy: 0.70 - ETA: 5:55 - loss: 0.8355 - accuracy: 0.70 - ETA: 5:55 - loss: 0.8342 - accuracy: 0.70 - ETA: 5:55 - loss: 0.8326 - accuracy: 0.70 - ETA: 5:55 - loss: 0.8315 - accuracy: 0.70 - ETA: 5:55 - loss: 0.8299 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8282 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8269 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8255 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8239 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8229 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8220 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8206 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8198 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8187 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8178 - accuracy: 0.70 - ETA: 5:54 - loss: 0.8164 - accuracy: 0.70 - ETA: 5:53 - loss: 0.8147 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8135 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8123 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8113 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8105 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8092 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8079 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8068 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8062 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8052 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8038 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8030 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8023 - accuracy: 0.71 - ETA: 5:53 - loss: 0.8010 - accuracy: 0.71 - ETA: 5:53 - loss: 0.7991 - accuracy: 0.71 - ETA: 5:52 - loss: 0.7986 - accuracy: 0.71 - ETA: 5:52 - loss: 0.7969 - accuracy: 0.71 - ETA: 5:52 - loss: 0.7954 - accuracy: 0.71 - ETA: 5:52 - loss: 0.7942 - accuracy: 0.71 - ETA: 5:52 - loss: 0.7933 - accuracy: 0.71 - ETA: 5:52 - loss: 0.7930 - accuracy: 0.71 - ETA: 5:52 - loss: 0.7920 - accuracy: 0.71 - ETA: 5:52 - loss: 0.7915 - accuracy: 0.71 - ETA: 5:52 - loss: 0.7904 - accuracy: 0.71 - ETA: 5:51 - loss: 0.7898 - accuracy: 0.71 - ETA: 5:51 - loss: 0.7891 - accuracy: 0.71 - ETA: 5:51 - loss: 0.7883 - accuracy: 0.72 - ETA: 5:51 - loss: 0.7871 - accuracy: 0.72 - ETA: 5:51 - loss: 0.7868 - accuracy: 0.72 - ETA: 5:51 - loss: 0.7858 - accuracy: 0.72 - ETA: 5:51 - loss: 0.7849 - accuracy: 0.72 - ETA: 5:50 - loss: 0.7833 - accuracy: 0.72 - ETA: 5:50 - loss: 0.7817 - accuracy: 0.72 - ETA: 5:50 - loss: 0.7809 - accuracy: 0.72 - ETA: 5:50 - loss: 0.7799 - accuracy: 0.72 - ETA: 5:50 - loss: 0.7790 - accuracy: 0.72 - ETA: 5:49 - loss: 0.7780 - accuracy: 0.72 - ETA: 5:49 - loss: 0.7767 - accuracy: 0.72 - ETA: 5:49 - loss: 0.7750 - accuracy: 0.72 - ETA: 5:49 - loss: 0.7743 - accuracy: 0.72 - ETA: 5:49 - loss: 0.7731 - accuracy: 0.72 - ETA: 5:49 - loss: 0.7722 - accuracy: 0.72 - ETA: 5:49 - loss: 0.7712 - accuracy: 0.72 - ETA: 5:48 - loss: 0.7700 - accuracy: 0.72 - ETA: 5:48 - loss: 0.7685 - accuracy: 0.72 - ETA: 5:48 - loss: 0.7676 - accuracy: 0.72 - ETA: 5:48 - loss: 0.7664 - accuracy: 0.72 - ETA: 5:48 - loss: 0.7656 - accuracy: 0.72 - ETA: 5:47 - loss: 0.7646 - accuracy: 0.72 - ETA: 5:47 - loss: 0.7632 - accuracy: 0.72 - ETA: 5:47 - loss: 0.7622 - accuracy: 0.72 - ETA: 5:47 - loss: 0.7616 - accuracy: 0.73 - ETA: 5:47 - loss: 0.7610 - accuracy: 0.73 - ETA: 5:47 - loss: 0.7596 - accuracy: 0.73 - ETA: 5:46 - loss: 0.7586 - accuracy: 0.73 - ETA: 5:46 - loss: 0.7574 - accuracy: 0.73 - ETA: 5:46 - loss: 0.7560 - accuracy: 0.73 - ETA: 5:46 - loss: 0.7548 - accuracy: 0.73 - ETA: 5:46 - loss: 0.7544 - accuracy: 0.73 - ETA: 5:45 - loss: 0.7538 - accuracy: 0.73 - ETA: 5:45 - loss: 0.7528 - accuracy: 0.73 - ETA: 5:45 - loss: 0.7515 - accuracy: 0.73 - ETA: 5:45 - loss: 0.7505 - accuracy: 0.73 - ETA: 5:45 - loss: 0.7495 - accuracy: 0.73 - ETA: 5:44 - loss: 0.7491 - accuracy: 0.73 - ETA: 5:44 - loss: 0.7481 - accuracy: 0.73 - ETA: 5:44 - loss: 0.7471 - accuracy: 0.73 - ETA: 5:44 - loss: 0.7463 - accuracy: 0.73 - ETA: 5:44 - loss: 0.7451 - accuracy: 0.73 - ETA: 5:43 - loss: 0.7444 - accuracy: 0.73 - ETA: 5:43 - loss: 0.7431 - accuracy: 0.73 - ETA: 5:43 - loss: 0.7421 - accuracy: 0.73 - ETA: 5:43 - loss: 0.7410 - accuracy: 0.73 - ETA: 5:43 - loss: 0.7404 - accuracy: 0.73 - ETA: 5:43 - loss: 0.7391 - accuracy: 0.73 - ETA: 5:43 - loss: 0.7380 - accuracy: 0.73 - ETA: 5:43 - loss: 0.7373 - accuracy: 0.73 - ETA: 5:43 - loss: 0.7365 - accuracy: 0.73 - ETA: 5:42 - loss: 0.7360 - accuracy: 0.73 - ETA: 5:42 - loss: 0.7353 - accuracy: 0.73 - ETA: 5:42 - loss: 0.7347 - accuracy: 0.74 - ETA: 5:42 - loss: 0.7339 - accuracy: 0.74 - ETA: 5:42 - loss: 0.7328 - accuracy: 0.74 - ETA: 5:42 - loss: 0.7319 - accuracy: 0.74 - ETA: 5:41 - loss: 0.7307 - accuracy: 0.74 - ETA: 5:41 - loss: 0.7301 - accuracy: 0.74 - ETA: 5:41 - loss: 0.7295 - accuracy: 0.74 - ETA: 5:41 - loss: 0.7289 - accuracy: 0.74 - ETA: 5:41 - loss: 0.7282 - accuracy: 0.74 - ETA: 5:41 - loss: 0.7272 - accuracy: 0.74 - ETA: 5:41 - loss: 0.7263 - accuracy: 0.74 - ETA: 5:41 - loss: 0.7255 - accuracy: 0.74 - ETA: 5:40 - loss: 0.7247 - accuracy: 0.74 - ETA: 5:40 - loss: 0.7237 - accuracy: 0.74 - ETA: 5:40 - loss: 0.7227 - accuracy: 0.74 - ETA: 5:40 - loss: 0.7217 - accuracy: 0.74 - ETA: 5:40 - loss: 0.7210 - accuracy: 0.74 - ETA: 5:40 - loss: 0.7200 - accuracy: 0.74 - ETA: 5:40 - loss: 0.7193 - accuracy: 0.74 - ETA: 5:40 - loss: 0.7184 - accuracy: 0.74 - ETA: 5:40 - loss: 0.7178 - accuracy: 0.74 - ETA: 5:39 - loss: 0.7167 - accuracy: 0.74 - ETA: 5:39 - loss: 0.7163 - accuracy: 0.74 - ETA: 5:39 - loss: 0.7157 - accuracy: 0.74 - ETA: 5:39 - loss: 0.7150 - accuracy: 0.7474"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82848/903798 [=>............................] - ETA: 5:39 - loss: 0.7138 - accuracy: 0.74 - ETA: 5:39 - loss: 0.7133 - accuracy: 0.74 - ETA: 5:39 - loss: 0.7127 - accuracy: 0.74 - ETA: 5:38 - loss: 0.7120 - accuracy: 0.74 - ETA: 5:38 - loss: 0.7112 - accuracy: 0.74 - ETA: 5:38 - loss: 0.7104 - accuracy: 0.74 - ETA: 5:38 - loss: 0.7095 - accuracy: 0.74 - ETA: 5:38 - loss: 0.7085 - accuracy: 0.74 - ETA: 5:37 - loss: 0.7078 - accuracy: 0.74 - ETA: 5:37 - loss: 0.7076 - accuracy: 0.74 - ETA: 5:37 - loss: 0.7069 - accuracy: 0.75 - ETA: 5:37 - loss: 0.7058 - accuracy: 0.75 - ETA: 5:37 - loss: 0.7053 - accuracy: 0.75 - ETA: 5:36 - loss: 0.7046 - accuracy: 0.75 - ETA: 5:36 - loss: 0.7041 - accuracy: 0.75 - ETA: 5:36 - loss: 0.7029 - accuracy: 0.75 - ETA: 5:36 - loss: 0.7022 - accuracy: 0.75 - ETA: 5:36 - loss: 0.7013 - accuracy: 0.75 - ETA: 5:36 - loss: 0.7006 - accuracy: 0.75 - ETA: 5:35 - loss: 0.6998 - accuracy: 0.75 - ETA: 5:35 - loss: 0.6990 - accuracy: 0.75 - ETA: 5:35 - loss: 0.6982 - accuracy: 0.75 - ETA: 5:35 - loss: 0.6977 - accuracy: 0.75 - ETA: 5:35 - loss: 0.6968 - accuracy: 0.75 - ETA: 5:35 - loss: 0.6962 - accuracy: 0.75 - ETA: 5:35 - loss: 0.6958 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6949 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6941 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6934 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6930 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6922 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6913 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6907 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6900 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6894 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6887 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6881 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6877 - accuracy: 0.75 - ETA: 5:34 - loss: 0.6871 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6864 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6856 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6850 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6845 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6839 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6833 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6824 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6820 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6813 - accuracy: 0.75 - ETA: 5:33 - loss: 0.6806 - accuracy: 0.75 - ETA: 5:32 - loss: 0.6801 - accuracy: 0.75 - ETA: 5:32 - loss: 0.6792 - accuracy: 0.76 - ETA: 5:32 - loss: 0.6786 - accuracy: 0.76 - ETA: 5:32 - loss: 0.6779 - accuracy: 0.76 - ETA: 5:32 - loss: 0.6774 - accuracy: 0.76 - ETA: 5:32 - loss: 0.6769 - accuracy: 0.76 - ETA: 5:32 - loss: 0.6760 - accuracy: 0.76 - ETA: 5:32 - loss: 0.6753 - accuracy: 0.76 - ETA: 5:31 - loss: 0.6747 - accuracy: 0.76 - ETA: 5:31 - loss: 0.6742 - accuracy: 0.76 - ETA: 5:31 - loss: 0.6735 - accuracy: 0.76 - ETA: 5:31 - loss: 0.6728 - accuracy: 0.76 - ETA: 5:31 - loss: 0.6722 - accuracy: 0.76 - ETA: 5:31 - loss: 0.6717 - accuracy: 0.76 - ETA: 5:31 - loss: 0.6711 - accuracy: 0.76 - ETA: 5:31 - loss: 0.6705 - accuracy: 0.76 - ETA: 5:31 - loss: 0.6701 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6698 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6698 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6690 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6685 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6681 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6675 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6672 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6666 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6661 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6657 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6652 - accuracy: 0.76 - ETA: 5:30 - loss: 0.6647 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6643 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6636 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6629 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6622 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6614 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6613 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6607 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6602 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6595 - accuracy: 0.76 - ETA: 5:29 - loss: 0.6589 - accuracy: 0.76 - ETA: 5:28 - loss: 0.6581 - accuracy: 0.76 - ETA: 5:28 - loss: 0.6573 - accuracy: 0.76 - ETA: 5:28 - loss: 0.6568 - accuracy: 0.76 - ETA: 5:28 - loss: 0.6563 - accuracy: 0.76 - ETA: 5:28 - loss: 0.6558 - accuracy: 0.76 - ETA: 5:28 - loss: 0.6551 - accuracy: 0.76 - ETA: 5:28 - loss: 0.6547 - accuracy: 0.76 - ETA: 5:28 - loss: 0.6540 - accuracy: 0.76 - ETA: 5:27 - loss: 0.6535 - accuracy: 0.76 - ETA: 5:27 - loss: 0.6529 - accuracy: 0.77 - ETA: 5:27 - loss: 0.6521 - accuracy: 0.77 - ETA: 5:27 - loss: 0.6517 - accuracy: 0.77 - ETA: 5:27 - loss: 0.6513 - accuracy: 0.77 - ETA: 5:27 - loss: 0.6507 - accuracy: 0.77 - ETA: 5:27 - loss: 0.6499 - accuracy: 0.77 - ETA: 5:27 - loss: 0.6494 - accuracy: 0.77 - ETA: 5:27 - loss: 0.6489 - accuracy: 0.77 - ETA: 5:26 - loss: 0.6481 - accuracy: 0.77 - ETA: 5:26 - loss: 0.6479 - accuracy: 0.77 - ETA: 5:26 - loss: 0.6472 - accuracy: 0.77 - ETA: 5:26 - loss: 0.6465 - accuracy: 0.77 - ETA: 5:26 - loss: 0.6464 - accuracy: 0.77 - ETA: 5:26 - loss: 0.6458 - accuracy: 0.77 - ETA: 5:25 - loss: 0.6450 - accuracy: 0.77 - ETA: 5:25 - loss: 0.6444 - accuracy: 0.77 - ETA: 5:25 - loss: 0.6437 - accuracy: 0.77 - ETA: 5:25 - loss: 0.6432 - accuracy: 0.77 - ETA: 5:25 - loss: 0.6424 - accuracy: 0.77 - ETA: 5:25 - loss: 0.6418 - accuracy: 0.77 - ETA: 5:24 - loss: 0.6410 - accuracy: 0.77 - ETA: 5:24 - loss: 0.6405 - accuracy: 0.77 - ETA: 5:24 - loss: 0.6398 - accuracy: 0.77 - ETA: 5:24 - loss: 0.6392 - accuracy: 0.77 - ETA: 5:24 - loss: 0.6386 - accuracy: 0.77 - ETA: 5:24 - loss: 0.6381 - accuracy: 0.77 - ETA: 5:24 - loss: 0.6374 - accuracy: 0.77 - ETA: 5:24 - loss: 0.6370 - accuracy: 0.77 - ETA: 5:23 - loss: 0.6365 - accuracy: 0.77 - ETA: 5:23 - loss: 0.6361 - accuracy: 0.77 - ETA: 5:23 - loss: 0.6355 - accuracy: 0.77 - ETA: 5:23 - loss: 0.6348 - accuracy: 0.77 - ETA: 5:23 - loss: 0.6342 - accuracy: 0.77 - ETA: 5:23 - loss: 0.6337 - accuracy: 0.77 - ETA: 5:23 - loss: 0.6333 - accuracy: 0.77 - ETA: 5:22 - loss: 0.6328 - accuracy: 0.77 - ETA: 5:22 - loss: 0.6321 - accuracy: 0.77 - ETA: 5:22 - loss: 0.6317 - accuracy: 0.77 - ETA: 5:22 - loss: 0.6311 - accuracy: 0.77 - ETA: 5:22 - loss: 0.6305 - accuracy: 0.77 - ETA: 5:22 - loss: 0.6300 - accuracy: 0.77 - ETA: 5:22 - loss: 0.6293 - accuracy: 0.77 - ETA: 5:21 - loss: 0.6287 - accuracy: 0.77 - ETA: 5:21 - loss: 0.6279 - accuracy: 0.77 - ETA: 5:21 - loss: 0.6276 - accuracy: 0.77 - ETA: 5:21 - loss: 0.6269 - accuracy: 0.77 - ETA: 5:21 - loss: 0.6269 - accuracy: 0.77 - ETA: 5:21 - loss: 0.6264 - accuracy: 0.77 - ETA: 5:21 - loss: 0.6259 - accuracy: 0.77 - ETA: 5:21 - loss: 0.6254 - accuracy: 0.77 - ETA: 5:20 - loss: 0.6249 - accuracy: 0.77 - ETA: 5:20 - loss: 0.6246 - accuracy: 0.78 - ETA: 5:20 - loss: 0.6243 - accuracy: 0.78 - ETA: 5:20 - loss: 0.6238 - accuracy: 0.78 - ETA: 5:20 - loss: 0.6232 - accuracy: 0.78 - ETA: 5:20 - loss: 0.6227 - accuracy: 0.78 - ETA: 5:20 - loss: 0.6223 - accuracy: 0.78 - ETA: 5:20 - loss: 0.6219 - accuracy: 0.78 - ETA: 5:20 - loss: 0.6214 - accuracy: 0.78 - ETA: 5:20 - loss: 0.6211 - accuracy: 0.78 - ETA: 5:20 - loss: 0.6208 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6203 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6198 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6195 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6190 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6185 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6180 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6174 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6171 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6166 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6164 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6160 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6155 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6149 - accuracy: 0.78 - ETA: 5:19 - loss: 0.6146 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6139 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6133 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6127 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6125 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6119 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6115 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6111 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6108 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6104 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6101 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6097 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6092 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6088 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6083 - accuracy: 0.7859"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 93120/903798 [==>...........................] - ETA: 5:18 - loss: 0.6078 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6074 - accuracy: 0.78 - ETA: 5:18 - loss: 0.6070 - accuracy: 0.78 - ETA: 5:17 - loss: 0.6065 - accuracy: 0.78 - ETA: 5:17 - loss: 0.6060 - accuracy: 0.78 - ETA: 5:17 - loss: 0.6057 - accuracy: 0.78 - ETA: 5:17 - loss: 0.6053 - accuracy: 0.78 - ETA: 5:17 - loss: 0.6049 - accuracy: 0.78 - ETA: 5:17 - loss: 0.6045 - accuracy: 0.78 - ETA: 5:17 - loss: 0.6041 - accuracy: 0.78 - ETA: 5:16 - loss: 0.6037 - accuracy: 0.78 - ETA: 5:16 - loss: 0.6034 - accuracy: 0.78 - ETA: 5:16 - loss: 0.6029 - accuracy: 0.78 - ETA: 5:16 - loss: 0.6025 - accuracy: 0.78 - ETA: 5:16 - loss: 0.6021 - accuracy: 0.78 - ETA: 5:16 - loss: 0.6017 - accuracy: 0.78 - ETA: 5:16 - loss: 0.6014 - accuracy: 0.78 - ETA: 5:16 - loss: 0.6010 - accuracy: 0.78 - ETA: 5:16 - loss: 0.6008 - accuracy: 0.78 - ETA: 5:15 - loss: 0.6003 - accuracy: 0.78 - ETA: 5:15 - loss: 0.5999 - accuracy: 0.78 - ETA: 5:15 - loss: 0.5995 - accuracy: 0.78 - ETA: 5:15 - loss: 0.5991 - accuracy: 0.78 - ETA: 5:15 - loss: 0.5988 - accuracy: 0.78 - ETA: 5:15 - loss: 0.5982 - accuracy: 0.78 - ETA: 5:15 - loss: 0.5977 - accuracy: 0.78 - ETA: 5:15 - loss: 0.5972 - accuracy: 0.79 - ETA: 5:15 - loss: 0.5969 - accuracy: 0.79 - ETA: 5:15 - loss: 0.5963 - accuracy: 0.79 - ETA: 5:15 - loss: 0.5958 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5954 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5951 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5948 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5945 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5941 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5938 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5935 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5932 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5928 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5925 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5922 - accuracy: 0.79 - ETA: 5:14 - loss: 0.5919 - accuracy: 0.79 - ETA: 5:13 - loss: 0.5914 - accuracy: 0.79 - ETA: 5:13 - loss: 0.5913 - accuracy: 0.79 - ETA: 5:13 - loss: 0.5910 - accuracy: 0.79 - ETA: 5:13 - loss: 0.5909 - accuracy: 0.79 - ETA: 5:13 - loss: 0.5904 - accuracy: 0.79 - ETA: 5:13 - loss: 0.5901 - accuracy: 0.79 - ETA: 5:13 - loss: 0.5897 - accuracy: 0.79 - ETA: 5:13 - loss: 0.5895 - accuracy: 0.79 - ETA: 5:13 - loss: 0.5891 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5886 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5885 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5882 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5879 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5875 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5870 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5865 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5861 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5857 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5852 - accuracy: 0.79 - ETA: 5:12 - loss: 0.5847 - accuracy: 0.79 - ETA: 5:11 - loss: 0.5842 - accuracy: 0.79 - ETA: 5:11 - loss: 0.5836 - accuracy: 0.79 - ETA: 5:11 - loss: 0.5833 - accuracy: 0.79 - ETA: 5:11 - loss: 0.5829 - accuracy: 0.7952WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eb416744970d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearlystop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, callbacks=[tensorboard, earlystop], validation_data = (x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
